Practical Machine Learning Project
Katya Huster

Saturday, November 21, 2015

Introduction

Given both training and test data from the following study we need to perform a research:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human 2013) . Stuttgart, Germany: ACM SIGCHI, 2013.
The goal is to use data from various devices of volunteer exercisers and to predict the manner in which they did the exercise.

Loading the data

First I load the libraries adn the data for the project.

train <-read.csv(file = "pml-training.csv", head = T, na.strings =  c("#DIV/0!", "", " ", "NA"))
test <- read.csv(file = "pml-testing.csv", head = T, na.strings = c("#DIV/0!", "", " ", "NA"))
library(caret)
## Warning: package 'caret' was built under R version 3.1.3
## Loading required package: lattice
## Loading required package: ggplot2
## Warning: package 'ggplot2' was built under R version 3.1.3
library(rpart.plot)
## Warning: package 'rpart.plot' was built under R version 3.1.3
## Loading required package: rpart
library(rattle)
## Warning: package 'rattle' was built under R version 3.1.3
## Rattle: A free graphical interface for data mining with R.
## Version 4.0.0 Copyright (c) 2006-2015 Togaware Pty Ltd.
## Type 'rattle()' to shake, rattle, and roll your data.
library(rpart)
library(randomForest)
## Warning: package 'randomForest' was built under R version 3.1.3
## randomForest 4.6-12
## Type rfNews() to see new features/changes/bug fixes.
Next is a quick overview of the data structure.

str(train[1:10])
## 'data.frame':    19622 obs. of  10 variables:
##  $ X                   : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ user_name           : Factor w/ 6 levels "adelmo","carlitos",..: 2 2 2 2 2 2 2 2 2 2 ...
##  $ raw_timestamp_part_1: int  1323084231 1323084231 1323084231 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 ...
##  $ raw_timestamp_part_2: int  788290 808298 820366 120339 196328 304277 368296 440390 484323 484434 ...
##  $ cvtd_timestamp      : Factor w/ 20 levels "02/12/2011 13:32",..: 9 9 9 9 9 9 9 9 9 9 ...
##  $ new_window          : Factor w/ 2 levels "no","yes": 1 1 1 1 1 1 1 1 1 1 ...
##  $ num_window          : int  11 11 11 12 12 12 12 12 12 12 ...
##  $ roll_belt           : num  1.41 1.41 1.42 1.48 1.48 1.45 1.42 1.42 1.43 1.45 ...
##  $ pitch_belt          : num  8.07 8.07 8.07 8.05 8.07 8.06 8.09 8.13 8.16 8.17 ...
##  $ yaw_belt            : num  -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 ...
names(train[1:5])
## [1] "X"                    "user_name"            "raw_timestamp_part_1"
## [4] "raw_timestamp_part_2" "cvtd_timestamp"
Cleaning the data

I see the first 8 variables are only for description purposes. I will take them out since they cannot be used as predictors.

#remove first 8 columns - description only
train<-train[, -c(1:8)]
#check for NA values >60% of the rows 
names <- c()
for (i in 1:length(train)){
    if(sum(is.na(train[,i])) >=.6){
        names <-append(names,colnames(train)[i])
    }
}

cleanTrain <- train[, -which(names(train) %in% names)]
Removing the NA columns helped eliminate around 100 columns

Making Training & Testing Subset

Since this is a relatively big data set with around 20,000 rows, I decided to split the initial train set into testing and training subset.

###split the train data into training and testing set
#leave the test set for validation
inTrain <- createDataPartition(y=cleanTrain$classe, p = .6, list = F)
training <- cleanTrain[inTrain,]
testing <- cleanTrain [- inTrain,]
rpart Model

As this is a classification task with categorical outcomes, I have chosen to test decision trees and random forests. I believe random forest should show better results, let’s find out, if random forest is really that better than decision trees.

set.seed(12321)
#create model with cross-validation for Classification  trees
modFit1<- train(classe~., data = training, method = "rpart", 
                trControl=trainControl(method = "cv", number = 4))
print(modFit1)
## CART 
## 
## 11776 samples
##    51 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (4 fold) 
## Summary of sample sizes: 8833, 8831, 8831, 8833 
## Resampling results across tuning parameters:
## 
##   cp          Accuracy   Kappa      Accuracy SD  Kappa SD  
##   0.03025629  0.5191089  0.3787006  0.01143998   0.02850265
##   0.03618889  0.4887047  0.3384476  0.01709020   0.03474623
##   0.06457641  0.3744906  0.1493074  0.10413575   0.17240675
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was cp = 0.03025629.
fancyRpartPlot(modFit1$finalModel)


After creating the model next step is to check how good it is performing against test data

## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 2027  666  673  568  350
##          B   33  510   43  227  308
##          C  117  133  561  178  175
##          D   50  209   91  313  204
##          E    5    0    0    0  405
## 
## Overall Statistics
##                                           
##                Accuracy : 0.4864          
##                  95% CI : (0.4752, 0.4975)
##     No Information Rate : 0.2845          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.3272          
##  Mcnemar's Test P-Value : < 2.2e-16       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9082   0.3360   0.4101  0.24339  0.28086
## Specificity            0.5980   0.9034   0.9069  0.91555  0.99922
## Pos Pred Value         0.4732   0.4550   0.4820  0.36101  0.98780
## Neg Pred Value         0.9424   0.8501   0.8792  0.86058  0.86054
## Prevalence             0.2845   0.1935   0.1744  0.16391  0.18379
## Detection Rate         0.2583   0.0650   0.0715  0.03989  0.05162
## Detection Prevalence   0.5460   0.1429   0.1484  0.11050  0.05226
## Balanced Accuracy      0.7531   0.6197   0.6585  0.57947  0.64004
##            
## prediction1    A    B    C    D    E
##           A 2027  666  673  568  350
##           B   33  510   43  227  308
##           C  117  133  561  178  175
##           D   50  209   91  313  204
##           E    5    0    0    0  405
In this case the model is not better than a flip of a coin, giving less than 50% accuracy. The model definitely needs improving. Both in sample and out of sample accuracy is less than 50%.

#check for near zero variance variables which are not good predictors
nzv <- nearZeroVar(training, saveMetrics = T)
nzv[nzv$nzv =="TRUE",]
## [1] freqRatio     percentUnique zeroVar       nzv          
## <0 rows> (or 0-length row.names)
Checking the near-zero variance didn’t help creating the right solution. Only one column has near zero variance, but it actually has zero-variance. Eliminating this column might not necessary improve the results.

randomForest

As the initial prediction model showed poor results, I’m moving to random Forests to test this model accuracy.

*** Note: I could not run a train model with random forest as a model. It would run for hours and then crush the sistem or return a memory error. However, running a randomForest function directly return the results in seconds.

set.seed(12321)
#create random forest model 

#modFit2 <- train(classe~., data = training, method = "rf", prox = T, 
#                 tuneGrid = data.frame(mtry=10), allowParallel=T, 
#                 trControl=trainControl(method = "cv", number = 3))
modfit2 <-randomForest(data = training, classe~., mtry = 3, type = 'classification')
print(modfit2)
## 
## Call:
##  randomForest(formula = classe ~ ., data = training, mtry = 3,      type = "classification") 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 3
## 
##         OOB estimate of  error rate: 0.92%
## Confusion matrix:
##      A    B    C    D    E class.error
## A 3339    5    2    0    2 0.002688172
## B   16 2257    6    0    0 0.009653357
## C    0   18 2034    2    0 0.009737098
## D    0    0   46 1881    3 0.025388601
## E    0    0    4    4 2157 0.003695150
At this time in sample error is about 1%. Let’s check the prediction accuracy.

#predict
prediction2 <- predict(modfit2, newdata = testing)
#check accuracy of prediction
confusionMatrix(prediction2, testing$classe)
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 2229   12    0    0    0
##          B    3 1495   11    0    0
##          C    0    9 1357   19    1
##          D    0    2    0 1263    3
##          E    0    0    0    4 1438
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9918          
##                  95% CI : (0.9896, 0.9937)
##     No Information Rate : 0.2845          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.9897          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9987   0.9848   0.9920   0.9821   0.9972
## Specificity            0.9979   0.9978   0.9955   0.9992   0.9994
## Pos Pred Value         0.9946   0.9907   0.9791   0.9961   0.9972
## Neg Pred Value         0.9995   0.9964   0.9983   0.9965   0.9994
## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
## Detection Rate         0.2841   0.1905   0.1730   0.1610   0.1833
## Detection Prevalence   0.2856   0.1923   0.1767   0.1616   0.1838
## Balanced Accuracy      0.9983   0.9913   0.9937   0.9907   0.9983
table(prediction2, testing$classe)
##            
## prediction2    A    B    C    D    E
##           A 2229   12    0    0    0
##           B    3 1495   11    0    0
##           C    0    9 1357   19    1
##           D    0    2    0 1263    3
##           E    0    0    0    4 1438
The random Forest model has 99.2% accuracy which is drastic improvement compared to Decision Trees.

Final testing of new 20 observations:

predictTest<- predict(modfit2, newdata = test)
table(predictTest)
## predictTest
## A B C D E 
## 7 8 1 1 3
Conclusion

The performed assignment showed that random forest is far more superior than Decision Trees model. However, it is also very computationally intence. Due to time limitation, I was not able to perform multiple model testing. Finding a model that can combime accuracy and timely implementation would be an interesting extention to this project.

As of the data and the results for prediction, please keep in mind this test was done using a very small sample of the population, and new data will most certainly bring the accuracy down. Looking into the results of prediction, it is great to see that there are ways to monitor the quality of fitness exsercises.
